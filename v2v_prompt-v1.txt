I want to build a direct speech-to-speech deep learning model that takes raw audio input and generates corresponding spoken audio output directly, without converting audio to text at any stage.

Please provide me with a comprehensive, step-by-step guide covering the following:

How to collect and prepare large-scale raw voice datasets, including paired input-output voice samples for supervised fine-tuning.

The best methods to convert raw audio waveforms into discrete audio tokens or representations suitable for model training.

How to pretrain a model (e.g., a neural codec language model or sequence-to-sequence transformer) on raw voice data to learn acoustic patterns, grammar, and linguistic meaning by predicting the next audio tokens autoregressively.

How to fine-tune this pretrained model on paired voice input-output data so it can learn to directly map spoken questions to spoken answers.

The inference process: how the model predicts output audio tokens autoregressively, and how to reconstruct output speech from tokens.

Methods to verify if the generated speech output is correct, and strategies to correct wrong outputs through additional training or fine-tuning.

Suggestions for real-time deployment with low latency, including model optimization techniques such as quantization or streaming generation.

The key functions, libraries, or frameworks useful at each stage.

How to continuously improve the system by collecting feedback and retraining.

Please explain the whole process end-to-end, focusing exclusively on direct raw audio input and output, and include practical tips, best practices, and a summarized workflow. Avoid any intermediate text conversion or speech recognition/transcription steps.

Additionally, please provide:

The full end-to-end code implementing this concept (not just sample snippets).

The recommended file and project structure.

How to prepare and format the dataset with raw input-output voice pairs.

Clear instructions on how to train the model (pretraining and fine-tuning).

How to test and monitor model performance during and after training.

How to handle incorrect predictions and improve model accuracy in a structured way.

Concept summary:
First, I want to pretrain the model on a large corpus of raw voice data using transformer layers (multi-head attention, self-attention) to predict the next audio token, enabling the model to learn the acoustic, linguistic, and semantic patterns in speech. Then, fine-tune the model with paired input-output voice samples to directly learn speech-to-speech mapping. During inference, the model should generate audio tokens autoregressively, reconstruct speech from tokens, and output spoken answers. I want to know how to verify outputs, fix errors if predictions are wrong, and continuously improve the system.